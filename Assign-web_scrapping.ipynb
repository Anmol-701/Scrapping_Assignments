{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8141f59-addd-4156-bd5e-4db60a230a2e",
   "metadata": {},
   "source": [
    "## Problem-1: What is web scrapping? Why is it used? Give three areas where web scrapping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11486ac8-027c-4b58-b4ed-326def70574a",
   "metadata": {},
   "source": [
    "### Web_scrapping:\n",
    "  - Process of extracting data from web sites.\n",
    "  - It Involves using software and tools to navigate the web page, retrieve the desired information and saved it in structured format like spreadsheet, database and JSON file etc.\n",
    "\n",
    "### Why is it used\n",
    "  - Data Collection and Analysis.\n",
    "  - Business Intelligence.\n",
    "  - Research and Academic Purposes.\n",
    "  \n",
    "### Three areas where web_scrapping is used to get data:\n",
    "  - E-commerce and Price Comparison.\n",
    "  - Socail Media anf Sentiment Analysis.\n",
    "  - Real Estate and property listings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aef34a-b646-4d07-8726-5259af8e3b06",
   "metadata": {},
   "source": [
    "## Problem-2: What are the different methods used for web scrapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cb5e1-57bf-47b8-b014-cba770556429",
   "metadata": {},
   "source": [
    "### 1. Manual Copy-Pasting:\n",
    "  - The simplest form of web scraping involves manually copying and pasting the required data from a website into a local file or document. While not automated, it's the most straightforward method.\n",
    "### 2.Beautiful Soup:\n",
    "  - A Python library for parsing HTML and XML documents and navigating the parse tree to extract specific data.\n",
    "### 3. Scrapy:\n",
    "  - An open-source web crawling framework that facilitates the creation of spiders to scrape websites efficiently and handle pagination, scraping rules, and data export."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16fc45-2ad5-4b87-9274-89924360f203",
   "metadata": {},
   "source": [
    "## Problem-3: What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e0485-5d68-4c02-ae2a-ca3731db2a31",
   "metadata": {},
   "source": [
    "  - Beautiful Soup is a Python library used for web scraping and parsing HTML or XML documents. It provides a convenient way to extract and navigate the data within these documents, allowing users to scrape information from web pages in a structured and efficient manner.\n",
    "  \n",
    "### Why is it used:\n",
    "  - Web Scraping:-  Beautiful Soup is primarily used for web scraping. It simplifies the process of extracting data from websites by providing a structured way to navigate and access the HTML content.\n",
    "  - Data Extraction and Parsing:-  It's used to parse HTML or XML documents, allowing for easy extraction of specific data like titles, paragraphs, links, images, or any other elements of interest from web pages.\n",
    "  - Data Cleaning and Preparation:-   Beautiful Soup is often used to clean and preprocess the scraped data, ensuring that it's structured and ready for further analysis or storage in databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216f8d9-cfec-422a-a605-ca6006800eaf",
   "metadata": {},
   "source": [
    "## Problem-4: Why is flask used in this web scrapping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0c209-2070-4645-9da2-c974da5b8dac",
   "metadata": {},
   "source": [
    "  - Flask is python web framework that is commonly used for building web applications and APIs.\n",
    "  - However, in the context of a web scraping project, Flask may not be directly related to the scraping process itself but rather used for creating a web application to display or interact with the scraped data.\n",
    "  - Flask is used so that to handle data storage, management and retrieval aspect of scrapped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2db07d-9ae4-44b1-a5bd-778fe4d2b1c9",
   "metadata": {},
   "source": [
    "## Project-5: Write the names of AWS services used in this project. Also, Explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62ba52-f7a5-40d6-b664-dbeec505b9f1",
   "metadata": {},
   "source": [
    "We used two services in this project \n",
    "  - Code pipeline\n",
    "  - Bean Stalk\n",
    "  \n",
    "### Code Pipeline:\n",
    "  - CodePipeline is a fully managed continuous delivery service that automates the steps required to release software changes continuously.\n",
    "  - It can be used to quickly release new features, iterate on feedback, and catch bugs by testing each code change.\n",
    "### Bean Stalk:\n",
    "  - AWS Elastic Beanstalk is a platform used for deploying and scaling web applications and services.\n",
    "  -  It provides an additional layer of abstraction over the bare server and OS, allowing users to see a pre-built combination of OS and platform.\n",
    "  - Elastic Beanstalk automatically handles the deployment, including capacity provisioning, load balancing, auto scaling, and application health monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba1c45-cb81-4549-a428-6e884863ffe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
